{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee58d047",
   "metadata": {},
   "source": [
    "# 🏁 Quick Start: EfficientNet Fine-tuning on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2c64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5d8c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "data_root = Path(\"../../../data/out_data_split\")\n",
    "train_dir = data_root / \"train\"\n",
    "val_dir = data_root / \"val\"\n",
    "batch_size = 32\n",
    "num_epochs = 30  # Keep it short for first run\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5f0e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['4011', '4015', '4088', '4196', '7020097009819', '7020097026113', '7023026089401', '7035620058776', '7037203626563', '7037206100022', '7038010009457', '7038010013966', '7038010021145', '7038010054488', '7038010068980', '7039610000318', '7040513000022', '7040513001753', '7040913336684', '7044610874661', '7048840205868', '7071688004713', '7622210410337', '90433917', '90433924', '94011']\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(str(train_dir), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(str(val_dir), transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa075d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"efficientnet_b0\", pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "model.classifier = nn.Linear(model.classifier.in_features, len(class_names))\n",
    "\n",
    "# Only train classifier\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2160726",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38000e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=0.2145, Val Acc=0.4682\n",
      "Epoch 2: Train Acc=0.5220, Val Acc=0.6182\n",
      "Epoch 3: Train Acc=0.6480, Val Acc=0.6894\n",
      "Epoch 4: Train Acc=0.7195, Val Acc=0.7621\n",
      "Epoch 5: Train Acc=0.7705, Val Acc=0.7909\n",
      "Epoch 6: Train Acc=0.8107, Val Acc=0.8197\n",
      "Epoch 7: Train Acc=0.8481, Val Acc=0.8470\n",
      "Epoch 8: Train Acc=0.8694, Val Acc=0.8652\n",
      "Epoch 9: Train Acc=0.8868, Val Acc=0.8803\n",
      "Epoch 10: Train Acc=0.9046, Val Acc=0.8879\n",
      "Epoch 11: Train Acc=0.9138, Val Acc=0.9045\n",
      "Epoch 12: Train Acc=0.9196, Val Acc=0.9091\n",
      "Epoch 13: Train Acc=0.9332, Val Acc=0.9167\n",
      "Epoch 14: Train Acc=0.9324, Val Acc=0.9212\n",
      "Epoch 15: Train Acc=0.9359, Val Acc=0.9273\n",
      "Epoch 16: Train Acc=0.9409, Val Acc=0.9318\n",
      "Epoch 17: Train Acc=0.9447, Val Acc=0.9364\n",
      "Epoch 18: Train Acc=0.9478, Val Acc=0.9364\n",
      "Epoch 19: Train Acc=0.9471, Val Acc=0.9333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m best_val_acc = \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     val_loss, val_acc = validate(model, val_loader)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader)\u001b[39m\n\u001b[32m      5\u001b[39m model.train()\n\u001b[32m      6\u001b[39m total_loss, correct = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/torchvision/transforms/functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"efficientnet_quickstart.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9ea9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         4011       1.00      0.62      0.77        24\n",
      "         4015       0.77      0.84      0.80        49\n",
      "         4088       0.81      0.94      0.87        36\n",
      "         4196       0.89      0.98      0.93        48\n",
      "7020097009819       0.65      1.00      0.79        37\n",
      "7020097026113       1.00      0.31      0.47        13\n",
      "7023026089401       1.00      0.90      0.95        20\n",
      "7035620058776       0.00      0.00      0.00         6\n",
      "7037203626563       0.00      0.00      0.00        10\n",
      "7037206100022       0.74      0.97      0.84        33\n",
      "7038010009457       1.00      0.79      0.88        14\n",
      "7038010013966       0.88      0.88      0.88        33\n",
      "7038010021145       1.00      0.86      0.92        14\n",
      "7038010054488       0.87      0.54      0.67        24\n",
      "7038010068980       0.91      0.97      0.94        33\n",
      "7039610000318       0.88      0.96      0.92        24\n",
      "7040513000022       0.76      0.93      0.84        28\n",
      "7040513001753       1.00      0.27      0.43        11\n",
      "7040913336684       1.00      0.64      0.78        14\n",
      "7044610874661       0.67      0.98      0.80        58\n",
      "7048840205868       1.00      0.21      0.35        14\n",
      "7071688004713       1.00      0.73      0.84        11\n",
      "7622210410337       1.00      0.53      0.70        15\n",
      "     90433917       0.60      0.16      0.25        19\n",
      "     90433924       0.59      0.70      0.64        27\n",
      "        94011       0.79      0.98      0.87        45\n",
      "\n",
      "     accuracy                           0.80       660\n",
      "    macro avg       0.80      0.68      0.70       660\n",
      " weighted avg       0.81      0.80      0.77       660\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/havardvd/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/havardvd/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/havardvd/git/Cogito/hackathon/4_apr/AST-NGD-COGITO-HACKATHON-2025/.venv/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(\"efficientnet_quickstart.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "        preds = out.argmax(1).cpu()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y)\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
